{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Prototype Model: Deteksi Code-Switching dengan Weak Supervision & Naive Bayes\n",
                "\n",
                "Notebook ini merupakan prototype untuk mendeteksi **Code-Switching** (percampuran bahasa Indonesia dan Inggris) pada tweet menggunakan pendekatan:\n",
                "1. **Weak Supervision** - Pelabelan data otomatis berbasis aturan (lexicon)\n",
                "2. **Naive Bayes + TF-IDF** - Model klasifikasi teks\n",
                "\n",
                "**Output:**\n",
                "- `dataset_hasil_pelabelan.csv` - Dataset dengan label bahasa\n",
                "- `confusion_matrix.png` - Visualisasi akurasi model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 1: Import Library\n",
                "\n",
                "Pada langkah ini, kita mengimpor semua library yang diperlukan:\n",
                "\n",
                "| Library | Fungsi |\n",
                "|---------|--------|\n",
                "| `pandas` | Manipulasi data tabular (DataFrame) |\n",
                "| `numpy` | Operasi numerik |\n",
                "| `re` | Regular Expression untuk text cleaning |\n",
                "| `matplotlib` & `seaborn` | Visualisasi data (confusion matrix) |\n",
                "| `sklearn` | Machine Learning (TF-IDF, Naive Bayes, evaluasi) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.pipeline import make_pipeline\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "\n",
                "print(\"[✓] Semua library berhasil diimpor!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 2: Fungsi Pembantu untuk Output\n",
                "\n",
                "Fungsi-fungsi ini digunakan untuk membuat output yang rapi dan mudah dibaca di terminal/console."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_header(title):\n",
                "    \"\"\"Membuat header dengan border untuk judul section\"\"\"\n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(f\" {title} \".center(70, \"=\"))\n",
                "    print(\"=\"*70)\n",
                "\n",
                "def print_separator():\n",
                "    \"\"\"Membuat garis pemisah\"\"\"\n",
                "    print(\"-\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# TAHAP 1: WEAK SUPERVISION (PELABELAN OTOMATIS)\n",
                "\n",
                "Weak Supervision adalah teknik pelabelan data secara otomatis menggunakan **aturan heuristik** (rules) tanpa perlu anotasi manual. Pada kasus ini, kita menggunakan **lexicon-based approach** untuk mengidentifikasi bahasa."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 3: Muat Dataset Mentah\n",
                "\n",
                "Dataset yang digunakan adalah `codeswitch_emotion.csv` yang berisi tweet dengan campuran bahasa Indonesia dan Inggris.\n",
                "\n",
                "**Catatan untuk Google Colab:** Upload file dataset terlebih dahulu."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# (Opsional) Upload file di Google Colab\n",
                "# from google.colab import files\n",
                "# uploaded = files.upload()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print_header(\"TAHAP 1: WEAK SUPERVISION (PELABELAN DATA OTOMATIS)\")\n",
                "\n",
                "try:\n",
                "    df = pd.read_csv('codeswitch_emotion.csv', on_bad_lines='skip')\n",
                "    print(f\"[INFO] Dataset 'codeswitch_emotion.csv' berhasil dimuat.\")\n",
                "    print(f\"[INFO] Total data mentah: {len(df)} baris.\")\n",
                "    display(df.head())\n",
                "except FileNotFoundError:\n",
                "    print(\"[ERROR] File dataset tidak ditemukan. Menggunakan data dummy.\")\n",
                "    df = pd.DataFrame({'tweet': [\n",
                "        \"Aku stuck banget sama deadline tugas\", \n",
                "        \"I love you so much\", \n",
                "        \"Makan nasi goreng enak di kantin\", \n",
                "        \"Which is sebenernya dia fine aja\"\n",
                "    ]})\n",
                "    display(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 4: Definisi Lexicon (Kamus Kata)\n",
                "\n",
                "Lexicon adalah kumpulan kata-kata kunci yang menjadi **penanda bahasa**. Kita mendefinisikan dua lexicon:\n",
                "\n",
                "1. **`indo_markers`** - Kata-kata penanda Bahasa Indonesia (termasuk slang seperti `gak`, `bgt`, `wkwk`)\n",
                "2. **`eng_markers`** - Kata-kata penanda Bahasa Inggris\n",
                "\n",
                "**Logika Pelabelan:**\n",
                "- Jika tweet mengandung kata dari **kedua** lexicon → `MIX` (Code-Switching)\n",
                "- Jika lebih banyak kata Inggris → `EN`\n",
                "- Selain itu → `ID`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Lexicon Bahasa Indonesia (termasuk slang dan singkatan)\n",
                "indo_markers = {\n",
                "    'aku', 'kamu', 'dia', 'kita', 'mereka', 'ini', 'itu', 'dan', 'atau', 'tapi',\n",
                "    'yang', 'di', 'ke', 'dari', 'bisa', 'mau', 'sudah', 'udah', 'lagi', 'lg',\n",
                "    'gak', 'ga', 'nggak', 'tak', 'jangan', 'sama', 'bgt', 'banget', 'dong', 'sih',\n",
                "    'kok', 'deh', 'kan', 'kalo', 'kalau', 'buat', 'utk', 'untuk', 'dgn', 'dengan',\n",
                "    'apa', 'kenapa', 'gimana', 'siapa', 'kapan', 'ya', 'yuk', 'wkwk', 'hehe', \n",
                "    'pake', 'pakai', 'ada', 'jadi', 'jd', 'bukan', 'krn', 'karena', 'yg', 'tidak'\n",
                "}\n",
                "\n",
                "# Lexicon Bahasa Inggris\n",
                "eng_markers = {\n",
                "    'i', 'you', 'he', 'she', 'we', 'they', 'it', 'this', 'that', 'and', 'or', 'but',\n",
                "    'which', 'who', 'what', 'where', 'when', 'why', 'how', 'is', 'am', 'are', 'was',\n",
                "    'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'can', 'could',\n",
                "    'will', 'would', 'should', 'to', 'of', 'in', 'on', 'at', 'for', 'with', 'by',\n",
                "    'from', 'about', 'just', 'like', 'so', 'not', 'no', 'yes', 'please', 'thanks',\n",
                "    'my', 'your', 'actually', 'literally', 'basically', 'prefer', 'guys', 'sorry'\n",
                "}\n",
                "\n",
                "print(f\"[INFO] Jumlah kata dalam lexicon Indonesia: {len(indo_markers)}\")\n",
                "print(f\"[INFO] Jumlah kata dalam lexicon Inggris  : {len(eng_markers)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 5: Fungsi Pelabelan Otomatis\n",
                "\n",
                "Fungsi `automated_labeling()` melakukan:\n",
                "1. **Preprocessing** - Ubah ke lowercase, hapus karakter non-alfabet\n",
                "2. **Tokenisasi** - Pecah teks menjadi kata-kata\n",
                "3. **Scoring** - Hitung berapa kata yang cocok dengan masing-masing lexicon\n",
                "4. **Labeling** - Tentukan label berdasarkan skor\n",
                "\n",
                "**Aturan Pelabelan:**\n",
                "\n",
                "| Kondisi | Label |\n",
                "|---------|-------|\n",
                "| Skor ID ≥ 1 **DAN** Skor EN ≥ 1 | `MIX` (Code-Switching) |\n",
                "| Skor EN > Skor ID | `EN` (English) |\n",
                "| Lainnya | `ID` (Indonesia) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def automated_labeling(text):\n",
                "    \"\"\"\n",
                "    Fungsi untuk melakukan pelabelan otomatis berdasarkan lexicon.\n",
                "    \n",
                "    Args:\n",
                "        text: String teks tweet\n",
                "    \n",
                "    Returns:\n",
                "        Label: 'ID', 'EN', atau 'MIX'\n",
                "    \"\"\"\n",
                "    # Handle non-string input\n",
                "    if not isinstance(text, str): \n",
                "        return 'ID'\n",
                "    \n",
                "    # Preprocessing: lowercase dan hapus karakter non-alfabet\n",
                "    text_clean = text.lower()\n",
                "    text_clean = re.sub(r'[^a-z\\s]', ' ', text_clean)\n",
                "    \n",
                "    # Tokenisasi\n",
                "    words = text_clean.split()\n",
                "    if len(words) == 0: \n",
                "        return 'ID'\n",
                "    \n",
                "    # Hitung skor berdasarkan intersection dengan lexicon\n",
                "    word_set = set(words)\n",
                "    id_score = len(word_set.intersection(indo_markers))\n",
                "    en_score = len(word_set.intersection(eng_markers))\n",
                "    \n",
                "    # Tentukan label\n",
                "    if id_score >= 1 and en_score >= 1: \n",
                "        return 'MIX'  # Code-Switching terdeteksi!\n",
                "    elif en_score > id_score: \n",
                "        return 'EN'\n",
                "    else: \n",
                "        return 'ID'\n",
                "\n",
                "# Contoh penggunaan\n",
                "print(\"[TEST] Contoh hasil pelabelan:\")\n",
                "test_texts = [\n",
                "    \"Aku stuck banget sama deadline\",\n",
                "    \"I love you so much\",\n",
                "    \"Makan nasi goreng enak\"\n",
                "]\n",
                "for t in test_texts:\n",
                "    print(f\"  '{t}' → {automated_labeling(t)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 6: Terapkan Pelabelan ke Seluruh Dataset\n",
                "\n",
                "Pada langkah ini, kita menerapkan fungsi `automated_labeling()` ke seluruh kolom `tweet` untuk menghasilkan kolom `label_bahasa`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[...] Sedang menjalankan algoritma pelabelan otomatis...\")\n",
                "df['label_bahasa'] = df['tweet'].apply(automated_labeling)\n",
                "print(\"[✓] Pelabelan selesai!\")\n",
                "\n",
                "# Tampilkan beberapa contoh hasil\n",
                "print(\"\\n[INFO] Contoh hasil pelabelan:\")\n",
                "display(df[['tweet', 'label_bahasa']].head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 7: Simpan Dataset Hasil Pelabelan\n",
                "\n",
                "Dataset yang sudah dilabeli disimpan ke file CSV untuk:\n",
                "- Dokumentasi di skripsi/laporan\n",
                "- Digunakan di tahap selanjutnya (training model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output_csv = 'dataset_hasil_pelabelan.csv'\n",
                "df[['tweet', 'label_bahasa']].to_csv(output_csv, index=False)\n",
                "print(f\"[✓] File '{output_csv}' berhasil disimpan! (Lampirkan di skripsi)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 8: Statistik Distribusi Label\n",
                "\n",
                "Menampilkan statistik distribusi label untuk memahami komposisi dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n[INFO] Statistik Distribusi Label:\")\n",
                "print_separator()\n",
                "print(f\"{'KELAS BAHASA':<15} | {'JUMLAH SAMPLE':<15} | {'PERSENTASE':<15}\")\n",
                "print_separator()\n",
                "\n",
                "stats = df['label_bahasa'].value_counts()\n",
                "total = len(df)\n",
                "for label, count in stats.items():\n",
                "    print(f\"{label:<15} | {count:<15} | {count/total:.2%}\")\n",
                "print_separator()\n",
                "\n",
                "# Visualisasi distribusi\n",
                "plt.figure(figsize=(8, 5))\n",
                "stats.plot(kind='bar', color=['#3498db', '#e74c3c', '#2ecc71'])\n",
                "plt.title('Distribusi Label Bahasa')\n",
                "plt.xlabel('Label')\n",
                "plt.ylabel('Jumlah')\n",
                "plt.xticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# TAHAP 2: PEMODELAN (NAIVE BAYES & TF-IDF)\n",
                "\n",
                "Pada tahap ini, kita melatih model klasifikasi menggunakan:\n",
                "- **TF-IDF Vectorizer** - Mengubah teks menjadi vektor numerik\n",
                "- **Multinomial Naive Bayes** - Algoritma klasifikasi yang cocok untuk data teks"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 9: Text Cleaning untuk Model\n",
                "\n",
                "Sebelum melatih model, kita melakukan preprocessing tambahan:\n",
                "1. Ubah ke lowercase\n",
                "2. Hapus placeholder `[username]` dan `[url]`\n",
                "3. Hapus karakter non-alfanumerik"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print_header(\"TAHAP 2: PELATIHAN MODEL NAIVE BAYES\")\n",
                "\n",
                "def clean_text_final(text):\n",
                "    \"\"\"\n",
                "    Fungsi untuk membersihkan teks sebelum diproses model.\n",
                "    \n",
                "    Args:\n",
                "        text: String teks mentah\n",
                "    \n",
                "    Returns:\n",
                "        String teks yang sudah dibersihkan\n",
                "    \"\"\"\n",
                "    text = str(text).lower()\n",
                "    text = re.sub(r'\\[username\\]|\\[url\\]', '', text)  # Hapus placeholder\n",
                "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Hapus karakter khusus\n",
                "    return text.strip()\n",
                "\n",
                "df['text_clean'] = df['tweet'].apply(clean_text_final)\n",
                "print(\"[✓] Text cleaning selesai!\")\n",
                "\n",
                "# Tampilkan contoh hasil cleaning\n",
                "print(\"\\n[INFO] Contoh hasil cleaning:\")\n",
                "display(df[['tweet', 'text_clean']].head(5))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 10: Split Data (Training & Testing)\n",
                "\n",
                "Dataset dibagi menjadi:\n",
                "- **80% Data Latih** - Untuk melatih model\n",
                "- **20% Data Uji** - Untuk mengevaluasi performa model\n",
                "\n",
                "**Parameter penting:**\n",
                "- `test_size=0.2` → 20% untuk testing\n",
                "- `random_state=42` → Reproducible split\n",
                "- `stratify=df['label_bahasa']` → Menjaga proporsi label di train dan test sama"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    df['text_clean'], \n",
                "    df['label_bahasa'], \n",
                "    test_size=0.2, \n",
                "    random_state=42,\n",
                "    stratify=df['label_bahasa']\n",
                ")\n",
                "\n",
                "print(f\"[INFO] Data Latih : {len(X_train)} sampel\")\n",
                "print(f\"[INFO] Data Uji   : {len(X_test)} sampel\")\n",
                "print(f\"[INFO] Rasio      : {len(X_train)/len(df):.0%} : {len(X_test)/len(df):.0%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 11: Buat dan Latih Model (Pipeline)\n",
                "\n",
                "Kita menggunakan **Pipeline** untuk menggabungkan:\n",
                "1. **TfidfVectorizer** - Mengubah teks → vektor TF-IDF\n",
                "   - `ngram_range=(1, 2)` → Menggunakan unigram dan bigram\n",
                "   - `max_features=5000` → Maksimal 5000 fitur\n",
                "2. **MultinomialNB** - Classifier Naive Bayes\n",
                "   - `alpha=0.1` → Smoothing parameter (Laplace smoothing)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = make_pipeline(\n",
                "    TfidfVectorizer(ngram_range=(1, 2), max_features=5000), \n",
                "    MultinomialNB(alpha=0.1)\n",
                ")\n",
                "\n",
                "print(\"[...] Sedang melatih model...\")\n",
                "model.fit(X_train, y_train)\n",
                "print(\"[✓] Model berhasil dilatih!\")\n",
                "\n",
                "# Info model\n",
                "print(\"\\n[INFO] Komponen Pipeline:\")\n",
                "for i, step in enumerate(model.steps):\n",
                "    print(f\"  {i+1}. {step[0]}: {type(step[1]).__name__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# TAHAP 3: EVALUASI & VISUALISASI\n",
                "\n",
                "Pada tahap ini, kita mengevaluasi performa model menggunakan:\n",
                "- **Accuracy Score** - Persentase prediksi yang benar\n",
                "- **Classification Report** - Precision, Recall, F1-Score per kelas\n",
                "- **Confusion Matrix** - Visualisasi kesalahan prediksi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 12: Prediksi dan Hitung Akurasi\n",
                "\n",
                "Model melakukan prediksi pada data uji, kemudian kita hitung akurasinya dengan rumus:\n",
                "\n",
                "$$\\text{Akurasi} = \\frac{\\text{Jumlah Prediksi Benar}}{\\text{Total Data Uji}} \\times 100\\%$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print_header(\"TAHAP 3: EVALUASI & HASIL\")\n",
                "\n",
                "y_pred = model.predict(X_test)\n",
                "acc_score = accuracy_score(y_test, y_pred)\n",
                "\n",
                "print(f\"\\n{' AKURASI MODEL ':*^40}\")\n",
                "print(f\"{acc_score:.2%}\".center(40))\n",
                "print(\"*\" * 40 + \"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 13: Classification Report\n",
                "\n",
                "Laporan klasifikasi memberikan metrik detail per kelas:\n",
                "\n",
                "| Metrik | Deskripsi |\n",
                "|--------|--------|\n",
                "| **Precision** | Dari semua prediksi kelas X, berapa yang benar? |\n",
                "| **Recall** | Dari semua data kelas X sebenarnya, berapa yang berhasil ditemukan? |\n",
                "| **F1-Score** | Harmonic mean dari Precision dan Recall |\n",
                "| **Support** | Jumlah sampel per kelas di data uji |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"[INFO] Detail Laporan Klasifikasi:\")\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 14: Confusion Matrix\n",
                "\n",
                "Confusion Matrix menunjukkan perbandingan antara **label sebenarnya** (sumbu Y) dengan **prediksi model** (sumbu X).\n",
                "\n",
                "- Diagonal utama (warna gelap) = Prediksi benar\n",
                "- Sel lainnya = Kesalahan prediksi (misclassification)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    cm = confusion_matrix(y_test, y_pred, labels=['ID', 'EN', 'MIX'])\n",
                "    \n",
                "    plt.figure(figsize=(8, 6))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "                xticklabels=['ID', 'EN', 'MIX'], \n",
                "                yticklabels=['ID', 'EN', 'MIX'])\n",
                "    plt.title('Confusion Matrix: Deteksi Code-Switching')\n",
                "    plt.ylabel('Label Asli (Weak Supervision)')\n",
                "    plt.xlabel('Prediksi Model')\n",
                "    plt.tight_layout()\n",
                "    \n",
                "    # Simpan gambar\n",
                "    plt.savefig('confusion_matrix.png', dpi=150)\n",
                "    print(\"[✓] Gambar 'confusion_matrix.png' berhasil disimpan.\")\n",
                "    \n",
                "    plt.show()\n",
                "except Exception as e:\n",
                "    print(f\"[WARNING] Gagal membuat gambar confusion matrix: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Step 15: (Opsional) Download File Hasil di Google Colab\n",
                "\n",
                "Uncomment kode di bawah untuk mendownload file hasil ke komputer lokal."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment untuk download file hasil di Google Colab\n",
                "# from google.colab import files\n",
                "# files.download('dataset_hasil_pelabelan.csv')\n",
                "# files.download('confusion_matrix.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Ringkasan\n",
                "\n",
                "Notebook ini telah menyelesaikan:\n",
                "\n",
                "1. ✅ **Weak Supervision** - Pelabelan otomatis 100% data menggunakan lexicon\n",
                "2. ✅ **Pemodelan** - Training Naive Bayes + TF-IDF\n",
                "3. ✅ **Evaluasi** - Akurasi, Classification Report, Confusion Matrix\n",
                "\n",
                "**File Output:**\n",
                "- `dataset_hasil_pelabelan.csv` - Dataset berlabel untuk dokumentasi\n",
                "- `confusion_matrix.png` - Visualisasi untuk skripsi/laporan"
            ]
        }
    ]
}